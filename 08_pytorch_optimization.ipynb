{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70262106",
   "metadata": {},
   "source": [
    "# PyTorch Optimization Techniques\n",
    "## Mini-batch gradient descent\n",
    "### Dataset preparation and batch loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ead1f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch sizes: [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 10]\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import torch\n",
    "from sklearn.datasets import load_wine\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load wine classification dataset (3 classes, 13 features)\n",
    "wine: Any = load_wine()\n",
    "X = torch.tensor(wine.data, dtype=torch.float32)\n",
    "y = torch.tensor(wine.target, dtype=torch.long)\n",
    "\n",
    "# Create dataset and dataloader for mini-batch processing\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader: Any = DataLoader(dataset, batch_size=12, shuffle=True)\n",
    "\n",
    "# Display batch sizes to verify mini-batch configuration\n",
    "print(f\"Batch sizes: {[len(item[0]) for item in dataloader]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286c2870",
   "metadata": {},
   "source": [
    "### Model training with mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5955de32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, loss: 0.26576852798461914\n",
      "Epoch: 200, loss: 0.1520654857158661\n",
      "Epoch: 300, loss: 0.1266169250011444\n",
      "Epoch: 400, loss: 0.10865620523691177\n",
      "Epoch: 500, loss: 0.11050953716039658\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define neural network architecture: 13 -> 16 -> 16 -> 3\n",
    "model_minibatch = nn.Sequential(\n",
    "    nn.Linear(13, 16), nn.ReLU(), nn.Linear(16, 16), nn.ReLU(), nn.Linear(16, 3)\n",
    ")\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_minibatch.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with mini-batch gradient descent\n",
    "num_epochs = 500\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    model_minibatch.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    # Process data in mini-batches\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass on mini-batch\n",
    "        batch_outputs = model_minibatch(X_batch)\n",
    "        batch_loss = criterion(batch_outputs, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss (weighted by batch size)\n",
    "        running_loss += batch_loss * X_batch.size(0)\n",
    "\n",
    "    # Calculate average epoch loss\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "\n",
    "    # Report progress every 100 epochs\n",
    "    if not (epoch + 1) % 100:\n",
    "        print(f\"Epoch: {epoch + 1}, loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b9f91",
   "metadata": {},
   "source": [
    "## Learning rate scheduling\n",
    "### Data preparation with train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d2e24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X size: torch.Size([142, 13])\n",
      "Validation X size: torch.Size([36, 13])\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import torch\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load wine classification dataset\n",
    "wine: Any = load_wine()\n",
    "X = torch.tensor(wine.data, dtype=torch.float32)\n",
    "y = torch.tensor(wine.target, dtype=torch.long)\n",
    "\n",
    "# Split data into training and validation sets (80/20 split)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Display dataset sizes for verification\n",
    "print(f\"Training X size: {X_train.size()}\")\n",
    "print(f\"Validation X size: {X_validation.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074abfc6",
   "metadata": {},
   "source": [
    "### Model training with adaptive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2842d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, loss: 1.1840826272964478, validation loss: 1.0858325958251953\n",
      "Epoch: 200, loss: 1.1472747325897217, validation loss: 1.059104323387146\n",
      "Epoch: 300, loss: 1.1240156888961792, validation loss: 1.046764850616455\n",
      "Epoch: 400, loss: 1.1114978790283203, validation loss: 1.0414541959762573\n",
      "Epoch: 500, loss: 1.1034529209136963, validation loss: 1.0368276834487915\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Define neural network architecture: 13 -> 16 -> 16 -> 3\n",
    "model_lrschedule = nn.Sequential(\n",
    "    nn.Linear(13, 16), nn.ReLU(), nn.Linear(16, 16), nn.ReLU(), nn.Linear(16, 3)\n",
    ")\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_lrschedule.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize learning rate scheduler (reduces LR when validation loss plateaus)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.1, patience=10\n",
    ")\n",
    "\n",
    "# Training loop with learning rate scheduling\n",
    "num_epochs = 500\n",
    "history = {\"loss\": [], \"val_loss\": []}\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model_lrschedule.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_lrschedule(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    history[\"loss\"].append(loss.item())\n",
    "\n",
    "    # Validation phase\n",
    "    model_lrschedule.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_outputs = model_lrschedule(X_validation)\n",
    "        validation_loss = criterion(validation_outputs, y_validation)\n",
    "\n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(validation_loss)\n",
    "        history[\"val_loss\"].append(validation_loss.item())\n",
    "\n",
    "    # Report progress every 100 epochs\n",
    "    if not (epoch + 1) % 100:\n",
    "        print(f\"Epoch: {epoch + 1}, loss: {loss}, validation loss: {validation_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b2a301",
   "metadata": {},
   "source": [
    "## Regularization techniques\n",
    "### Dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4bb599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=13, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.2, inplace=False)\n",
      "  (6): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define neural network architecture with dropout regularization: 13 -> 16 -> 16 -> 3\n",
    "model_dropout = nn.Sequential(\n",
    "    nn.Linear(13, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),  # Dropout with 20% probability after first hidden layer\n",
    "    nn.Linear(16, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),  # Dropout with 20% probability after second hidden layer\n",
    "    nn.Linear(16, 3),\n",
    ")\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_dropout.parameters(), lr=0.001)\n",
    "\n",
    "print(model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319ab74",
   "metadata": {},
   "source": [
    "### Training with dropout and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe51510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200, loss: 1.0045427083969116, validation loss: 0.8699984550476074\n",
      "Epoch: 400, loss: 0.9233108162879944, validation loss: 0.7323180437088013\n",
      "Epoch: 500, L2 regularization enabled\n",
      "Epoch: 600, loss: 0.8085758686065674, validation loss: 0.6503400802612305\n",
      "Epoch: 800, loss: 0.6585947871208191, validation loss: 0.5298316478729248\n",
      "Epoch: 1000, loss: 0.5619053244590759, validation loss: 0.40555843710899353\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Training loop with combined dropout and L2 regularization\n",
    "num_epochs = 1000\n",
    "history = {\"loss\": [], \"val_loss\": []}\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase with dropout enabled\n",
    "    model_dropout.train()  # Enables dropout during training\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_dropout(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    history[\"loss\"].append(loss.item())\n",
    "\n",
    "    # Validation phase with dropout disabled\n",
    "    model_dropout.eval()  # Disables dropout during evaluation\n",
    "    with torch.no_grad():\n",
    "        validation_outputs = model_dropout(X_validation)\n",
    "        validation_loss = criterion(validation_outputs, y_validation)\n",
    "        history[\"val_loss\"].append(validation_loss.item())\n",
    "\n",
    "    # Enable L2 regularization halfway through training\n",
    "    if (epoch + 1) == 500:\n",
    "        # Add weight decay (L2 regularization) to optimizer\n",
    "        optimizer = optim.Adam(model_dropout.parameters(), lr=0.001, weight_decay=0.02)\n",
    "        print(f\"Epoch: {epoch + 1}, L2 regularization enabled\")\n",
    "\n",
    "    # Report progress every 200 epochs\n",
    "    if not (epoch + 1) % 200:\n",
    "        print(f\"Epoch: {epoch + 1}, loss: {loss}, validation loss: {validation_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
